# NOTE

## Vision 

### Lecture2

[ ] Neuron

-- [ ] ion channel

```
sodium/potassium; depolarization; synaptic; neurotransmitter; photoreceptors; cones; rods; perception; ganglion; potential; 
```

[ ] Excitatory post-synaptic potentials (EPSPs)

[ ] Inhibit post-synaptic potentials (IPSPs)

```
Both EPSPs and IPSPs is a part of threshold operation. When neurontransmitter reach the post-synaptic receptor, it cause the recptor's ion channel to open. The membrane potential will increase after receptor's ion channel is open. Over time the depolarization decreases since the sodium and potassium are pumped around to return to rest potential.
But when the membrane potential reach the threshold, a more extreme depolarization will happen. This will be pass down to the nect layer of neurons.
As it is in artificial neuron networks, IPSP and EPSP produce negative and positive response on postsynaptic neuron.
```

[ ] weight in neurons

```
So the weights on filters result primarily 
from learning.
```

[ ] Input feature maps

```
The four types of photoreceptors includes three color sensitive cones and one type of rod sensor.
```



[ ] spatial comparisons

-- [ ] surround suppression

```
THe retina correspense to a batch of nearby photoreceptor cells. Central cells produce EPSP and surrounding cells produce IPSP, so activation at the edges inhibits response to activation at the centre. This is called a ON response.
The s condurround suppression convert the light level into amount of contrast 
```

-- [ ] Normalization

```
The light level is used to normalise the light level in receptive field. The normalizaion is locally performed .
The 
```

[ ] receptive field

```
The part of input image that have a positive response in a neuron.
```

[ ] color comparisons

[ ] Receptive field extent and  spatial frequency

``` 
The spatial extent of integrating filters increases form cetral to peripheral vision. Hence, using different size of receptive fields, the retina can produce different component of different spatial scale. So the filters in retina perform a range of comparison operations on their inputs.
```

[ ] Hebbian learning

```
Neurons that fire together, wire together.
Strengthens responses to patterns of activity previously causing with responses.
```



### Lecture3

[ ] Orientation selectivity

```
Constrast is initially computed in an orientation-independent filter.
Orientation elective response are computed in V1 by operations by operations comparing these retinal ganglion cell outputs.
```

[ ] Spatial integration through the hierarchy

```
The receptive field in higher visual areas are larger than primary visual area(V1), visual cortical areas beyond V1 also map the spatial visual field onto the surface of cortex.
There are multiple branching hierarchies of these visual field maps associated with different tasks.
Main division into ventral and dorsal streams.

```

[ ] Intermixed feature representations

```
These different elements effectively form several distinct feature maps that are all mixed together at same cortica location.
Including Color, eye, spatial frequency, Orientation, Motion direction
```

[ ] Distributed encoding

```
Object recognition.
Representation still works if some cells die.
Can store new patterns without new cells.
Consist with measured cell properties.
```

[ ] Ventral and Dorsal stream

```
Ventral stream is associated with object recognition and object representation.
Dorsal stream is associated with motion and location, also used in controling eyes and arms.
```

[ ] Feature transformations through the visual hierarchy

```
Transformations find commonly-seen patterns in activity of earlier layers.
Later stages are likely doing the same computation, but have more abstract inputs.
Later in ventral stream, face-selective neurons are found.
```

[ ] Object-selectivity, imagination, attention

```
Response to many classes of object.(human faces, words and tools)
Silimar responses recently found for sementic content in language.
Responses can be driven by imagined content.
Responses are drawn towards attended locations.
```

### Lecture4

[ ] recurrent processing

```
Important properties of early and higher visual processing are absent in current artificial DCCN.
Give artificial DCNNs new abilities.
Make artificial DCNNs more brain-like.
Unite Hebbian learning and backpropagation.
Reduce the gap of biological and artificial learning.
```

[ ] Feedback, lateral and  recurrent activity in the brain

```
The brain heavily relies on lateral and feedback activity.
Recently developed recurrent convolutional artificial network are investigating lateral and top-down connection.
```

[ ] Attractor/Hopfield networks

```
Recurrent connections effectively make networks deeper.
- same transformation repeated by recurrent cycles.
- Each layer performs mulitple layer operations.
- matched neural architecture and activity more closely than DNN.
Attractor/Hopfield network among lateral connections.
- Attract activity patterns towrads previously common states.
- 
```

[ ] Predictive coding

```
predictions of incomplete or upcoming input in attractor/Hopfield network layers.

```





## Language

